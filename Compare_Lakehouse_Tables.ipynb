{"cells":[{"cell_type":"code","source":["# PARAMETERS \n","# Provide two workspaces and two lakehouses names\n","\n","workspace_name_1 = ''       # <----- FILL\n","lakehouse_name_1 = ''       # <----- FILL\n","\n","workspace_name_2 = ''       # <----- FILL\n","lakehouse_name_2 = ''       # <----- FILL\n","\n","config_param_comparison_mode = \"structure\"                  # <----- FILL\n","# Which dataframes to display as a result?\n","# rowscols - table_name, rowcount_1, rowcount_2, rowscount_match, colscount_1, colscount_2, colscount_match |||||| structure - table_name, column_name, type_1, type_2, type_match\n","# ### OPTIONS: rowscols (default)    | structure | full\n","\n","config_param_shortcuts = \"included\"                         # <----- FILL\n","# Do you want to include shortcuts or scan tables only?\n","# ### OPTIONS: included (default)    | excluded  | only\n","\n","config_param_results_match = \"all\"                          # <----- FILL\n","# Do you want to return all records, or no matches (on rows count, cols count or type checks)?\n","# ### OPTIONS: all (default)         | nomatchonly\n","\n","config_param_item_filter = \"e%\"                             # <----- FILL\n","# Do you want to scan all tables/shortcut names or only specified?\n","# ### OPTIONS: empty (default)       | custom table name, e.g. charges ; multiple table names separated with , ; regular expressions work as well\n","\n","config_param_progress_log = \"enabled\"                       # <----- FILL\n","# Do you want to show the progress log?\n","# ### OPTIONS: enabled (default)     | disabled"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"3b680336-1b98-400a-ac48-6bb344c64b8b"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","import json\n","import re\n","from delta.tables import DeltaTable\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n","from pyspark.sql import Row, DataFrame\n","from pyspark.sql.functions import col, lower, array_except, lit, split\n","from pyspark.sql import types\n","\n","# Function returning arrays operation result\n","def select_array(array1, array2, param):\n","    if param == \"included\":\n","        return array1\n","    elif param == \"only\":\n","        return array2\n","    elif param == \"excluded\":\n","        return list(set(array1) - set(array2))  # Remove items in array2 from array1\n","    else:\n","        return array1\n","\n","# Function filtering lakehouse items\n","def filter_lakehouse_tables(lakehouse_items_names, config_param_item_filter):\n","    # If filter is empty, return all table names\n","    if not config_param_item_filter.strip():\n","        return lakehouse_items_names\n","\n","    # Split filters by commas and convert SQL-like % to regex-compatible .*\n","    filters = [f.strip().replace(\"%\", \".*\") for f in config_param_item_filter.split(\",\")]\n","\n","    # Apply regex filtering\n","    filtered_tables = [table for table in lakehouse_items_names if any(re.fullmatch(pattern, table) for pattern in filters)]\n","\n","    return filtered_tables\n","\n","\n","# Create a function to get LH delta tables stats\n","def get_lh_table_data_stats(workspace_name: str, lakehouse_name: str):\n","\n","    # Define the schema for rowscols\n","    schema_df_rowscols = StructType([\n","        StructField(\"workspace_name\", StringType(), True),\n","        StructField(\"lakehouse_name\", StringType(), True),\n","        StructField(\"table_name\", StringType(), True),\n","        StructField(\"rows_cnt\", IntegerType(), True),\n","        StructField(\"cols_cnt\", IntegerType(), True)\n","    ])\n","\n","    # Define the schema for structure\n","    schema_df_structure = StructType([\n","        StructField(\"name_col\", StringType(), True),\n","        StructField(\"type_col\", StringType(), True)\n","    ])\n","\n","    # Create an empty DataFrame with the defined schema\n","    df_rowscols = spark.createDataFrame([], schema_df_rowscols)    \n","\n","    # Create an empty DataFrame with the defined schema\n","    df_structure = spark.createDataFrame([], schema_df_structure)   \n","\n","    # Extract a list of all environments\n","    workspace_raw = fabric.FabricRestClient().get(f\"/v1/workspaces\").json()\n","    workspaces = workspace_raw.get(\"value\", [])\n","    workspaces_df = spark.createDataFrame(workspaces)\n","\n","    # Find workspace id\n","    selected_workspace_name = workspace_name\n","    selected_workspace_df = workspaces_df.filter(workspaces_df[\"displayName\"] == selected_workspace_name)\n","    selected_workspace_id_row = selected_workspace_df.select(\"id\").collect()\n","    selected_workspace_id = selected_workspace_id_row[0][\"id\"]\n","\n","    # Extract a list of all items in workspace\n","    workspace_items_path = \"v1/workspaces/\"+selected_workspace_id+\"/items\"\n","    workspace_items_raw = fabric.FabricRestClient().get(workspace_items_path).json()\n","    workspace_items = workspace_items_raw.get(\"value\", [])\n","    workspace_items_df = spark.createDataFrame(workspace_items)\n","\n","    # Find lakehouse id\n","    lakehouses_df = workspace_items_df.filter((workspace_items_df[\"displayName\"] == lakehouse_name) & (workspace_items_df[\"type\"] == 'Lakehouse'))\n","    lakehouse_id_row = lakehouses_df.select(\"id\").collect()\n","    lakehouse_id = lakehouse_id_row[0][\"id\"]\n","\n","    # Extract a list of all tables in lakehouse\n","    lakehouse_items_path = \"v1/workspaces/\"+selected_workspace_id+\"/lakehouses/\"+lakehouse_id+\"/tables\"\n","    lakehouse_items_raw = fabric.FabricRestClient().get(lakehouse_items_path).json()\n","    lakehouse_tables_names = [item['name'] for item in lakehouse_items_raw['data']]\n","\n","    # Extract a list of all shortcuts in lakehouse\n","    shortcuts_list = \"v1/workspaces/\"+selected_workspace_id+\"/items/\"+lakehouse_id+\"/shortcuts\"\n","    shortcuts_list_raw = fabric.FabricRestClient().get(shortcuts_list).json()\n","    shortcuts_list_names = [item['name'] for item in shortcuts_list_raw['value']]\n","    # df_shortcuts_list_raw = spark.createDataFrame(shortcuts_list_raw[\"value\"])\n","    # df_shortcut_list = df_shortcuts_list_raw.select(col(\"name\"))\n","\n","    # Apply config_param_shortcuts value\n","    lakehouse_tables_names = select_array(lakehouse_tables_names, shortcuts_list_names, config_param_shortcuts)\n","\n","    # Apply filtering on tables names\n","    lakehouse_tables_names = filter_lakehouse_tables(lakehouse_tables_names, config_param_item_filter)\n","\n","    # Iterator built\n","    item_count = len(lakehouse_tables_names)\n","    iterator = 1\n","\n","    # Apply progress log display\n","    if config_param_progress_log == \"enabled\":\n","        display(f\"{workspace_name} - {lakehouse_name} lakehouse\")\n","\n","    for table_item in lakehouse_tables_names:\n","        \n","        # Define delta table path\n","        delta_table_path = (f\"abfss://{selected_workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Tables/{table_item}\")\n","\n","        # Check DeltaTable params remotely\n","        try:\n","            delta_table = DeltaTable.forPath(spark, delta_table_path)\n","        except AnalysisException as e:\n","            if \"DELTA_MISSING_DELTA_TABLE\" in str(e):\n","                display(f\"Skipping non-Delta table: {delta_table_path}\")\n","                delta_table = None  # or continue if inside a loop\n","            else:\n","                raise  # Re-raise other exceptions\n","\n","        if config_param_comparison_mode != \"structure\":\n","\n","            # Count rows and columns\n","            row_count = delta_table.toDF().count()          # Optimized rows count\n","            column_count = len(delta_table.toDF().columns)  # Optimized cols count\n","\n","            # Create a new row to insert into result\n","            new_row = Row(workspace_name = workspace_name, lakehouse_name = lakehouse_name, table_name = table_item, rows_cnt = row_count, cols_cnt = column_count)\n","            new_row_df = spark.createDataFrame([new_row], schema_df_rowscols)\n","            df_rowscols = df_rowscols.union(new_row_df)\n","\n","        if config_param_comparison_mode != \"rowscols\":\n","\n","            # Create a new row to insert schema info\n","            schema_info = [(field.name, field.dataType.simpleString()) for field in delta_table.toDF().schema.fields]\n","            new_row_df = spark.createDataFrame(schema_info, schema_df_structure)\n","            new_row_df = new_row_df.withColumn(\n","                \"name_col\",  # Name of the column you want to update\n","                F.concat(F.lit(table_item + \".\"), F.col(\"name_col\"))  # Concatenate table_item + \".\" with the existing name_col value\n","                )\n","            df_structure = df_structure.union(new_row_df)\n","\n","        # Iterator increase\n","        if config_param_progress_log == \"enabled\":\n","            display(f\"({iterator}/{item_count}) {table_item} processed\")\n","\n","        iterator += 1\n","    \n","    if config_param_progress_log == \"enabled\":\n","        display(\"---\")\n","    return df_rowscols, df_structure\n","\n","# Call the function twice\n","ws1_result, ws1_result_structures = get_lh_table_data_stats(workspace_name_1, lakehouse_name_1)\n","ws2_result, ws2_result_structures = get_lh_table_data_stats(workspace_name_2, lakehouse_name_2)\n","\n","# Format the partialresults\n","ws1_result = ws1_result.withColumn(\"table_name\", lower(col(\"table_name\")))\n","ws1_result = ws1_result.withColumnRenamed(\"rows_cnt\", f\"ROWS__{workspace_name_1}__{lakehouse_name_1}\")\n","ws1_result = ws1_result.withColumnRenamed(\"cols_cnt\", f\"COLS__{workspace_name_1}__{lakehouse_name_1}\")\n","\n","ws1_result_structures = ws1_result_structures.withColumn(\"name_col\", lower(col(\"name_col\")))\n","ws1_result_structures = ws1_result_structures.withColumnRenamed(\"type_col\", f\"TYPE__{workspace_name_1}__{lakehouse_name_1}\")\n","\n","ws2_result = ws2_result.withColumn(\"table_name\", lower(col(\"table_name\")))\n","ws2_result = ws2_result.withColumnRenamed(\"rows_cnt\", f\"ROWS__{workspace_name_2}__{lakehouse_name_2}\")\n","ws2_result = ws2_result.withColumnRenamed(\"cols_cnt\", f\"COLS__{workspace_name_2}__{lakehouse_name_2}\")\n","\n","ws2_result_structures = ws2_result_structures.withColumn(\"name_col\", lower(col(\"name_col\")))\n","ws2_result_structures = ws2_result_structures.withColumnRenamed(\"type_col\", f\"TYPE__{workspace_name_2}__{lakehouse_name_2}\")\n","\n","ws1_result = ws1_result.drop(\"workspace_name\", \"lakehouse_name\")\n","ws2_result = ws2_result.drop(\"workspace_name\", \"lakehouse_name\")\n","\n","# Join (merge) the results\n","df_result_rowscols = ws1_result.join(ws2_result, on=\"table_name\", how=\"outer\")\n","df_result_structure = ws1_result_structures.join(ws2_result_structures, on=\"name_col\", how=\"outer\")\n","\n","# Create match columns\n","df_result_rowscols = df_result_rowscols.withColumn(\n","    \"ROWS_MATCH\", \n","    F.when(F.col(f\"ROWS__{workspace_name_1}__{lakehouse_name_1}\") == F.col(f\"ROWS__{workspace_name_2}__{lakehouse_name_2}\"), True).otherwise(False)\n",")\n","\n","df_result_rowscols = df_result_rowscols.withColumn(\n","    \"COLS_MATCH\", \n","    F.when(F.col(f\"COLS__{workspace_name_1}__{lakehouse_name_1}\") == F.col(f\"COLS__{workspace_name_2}__{lakehouse_name_2}\"), True).otherwise(False)\n",")\n","\n","df_result_structure = df_result_structure.withColumn(\n","    \"TYPE_MATCH\", \n","    F.when(F.col(f\"TYPE__{workspace_name_1}__{lakehouse_name_1}\") == F.col(f\"TYPE__{workspace_name_2}__{lakehouse_name_2}\"), True).otherwise(False)\n",")\n","\n","# Split name_col column in structure df into two new columns and drop the original\n","df_result_structure = df_result_structure.withColumn(\"table_name\", split(col(\"name_col\"), \"\\\\.\").getItem(0)) \\\n","       .withColumn(\"column_name\", split(col(\"name_col\"), \"\\\\.\").getItem(1)) \\\n","       .drop(\"name_col\")  # Remove the original column\n","\n","# Format and display result\n","df_result_rowscols = df_result_rowscols.select(\"table_name\", f\"ROWS__{workspace_name_1}__{lakehouse_name_1}\", f\"ROWS__{workspace_name_2}__{lakehouse_name_2}\", \"ROWS_MATCH\", f\"COLS__{workspace_name_1}__{lakehouse_name_1}\", f\"COLS__{workspace_name_2}__{lakehouse_name_2}\", \"COLS_MATCH\")\n","df_result_structure = df_result_structure.select(\"table_name\", \"column_name\", f\"TYPE__{workspace_name_1}__{lakehouse_name_1}\", f\"TYPE__{workspace_name_2}__{lakehouse_name_2}\", \"TYPE_MATCH\")\n","\n","# Apply filtering on results match\n","if config_param_results_match == \"nomatchonly\":\n","    df_result_rowscols = df_result_rowscols.filter((col(\"ROWS_MATCH\") == False) | (col(\"COLS_MATCH\") == False))\n","    df_result_structure = df_result_structure.filter(col(\"TYPE_MATCH\") == False)\n","\n","# Display result\n","if config_param_comparison_mode == \"rowscols\":\n","    display(df_result_rowscols)\n","elif config_param_comparison_mode == \"structure\":\n","    display(df_result_structure)\n","elif config_param_comparison_mode == \"full\":\n","    display(df_result_rowscols)\n","    display(df_result_structure)\n","else:\n","    display(df_result_rowscols)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"16328036-dfb4-47b5-a774-165f553adf1f"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}